dashboard:
  image:
    tag: 1.2
guardrails-usvc:
  image:
    tag: 1.2
llm-uservice:
  image:
    tag: 1.2
embedding-usvc:
  image:
    tag: 1.2
reranking-usvc:
  image:
    tag: 1.2
chatqna-ui:
  image:
    tag: 1.2
nginx:
  image:
    tag: 1.2
image:
  tag: 1.2

tgi:
  enabled: false
vllm:
  enabled: false

tei:
  EMBEDDING_MODEL_ID: "/data/BAAI/bge-base-en-v1.5"
teirerank:
  RERANK_MODEL_ID: "/data/BAAI/bge-reranker-base"

ipexllm:
  enabled: true
  shmSize: 8Gi
  DTYPE: float16
  QUANTIZATION: fp8
  # Please adjust the model along with the followings:
  # - TENSOR_PARALLEL_SIZE: use multiple GPU cards for TP serving, usually for models with more than 10B.
  # - resource limits: should be at least equal to TENSOR_PARALLEL_SIZE * PIPELINE_PARALLEL_SIZE, also should be large enough to load the whole model
  # See https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md#multi-card-serving for details
  LLM_MODEL_ID: "/data/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
  TENSOR_PARALLEL_SIZE: 1
  resources:
    limits:
      gpu.intel.com/i915: 1

data-prep:
  image:
    tag: pre-1.3
  offline: true
  # set nltkDataUseHostPath or nltkDataUsePVC to use nltkData cache.
  nltkDataUseHostPath: /opt/opea-offline-data/nltk_data
  # nltkDataUsePVC: nltkData-PVC

retriever-usvc:
  image:
    tag: 1.2
  # set nltkDataUseHostPath or nltkDataUsePVC to use nltkData cache.
  nltkDataUseHostPath: /opt/opea-offline-data/nltk_data
  # nltkDataUsePVC: nltkData-PVC

global:
  HUGGINGFACEHUB_API_TOKEN: "insert-your-huggingface-token-here"
  # set modelUseHostPath or modelUsePVC to use model cache.
  modelUseHostPath: /opt/opea-offline-data/models
  # modelUsePVC: model-PVC
