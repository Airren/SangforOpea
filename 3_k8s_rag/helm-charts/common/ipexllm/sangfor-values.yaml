shmSize: 16Gi
DTYPE: float16
QUANTIZATION: fp8
# Please adjust the model along with the followings:
# - TENSOR_PARALLEL_SIZE: use multiple GPU cards for TP serving, usually for models with more than 10B.
# - resource limits: should be at least equal to TENSOR_PARALLEL_SIZE * PIPELINE_PARALLEL_SIZE, also should be large enough to load the whole model
# See https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md#multi-card-serving for details
LLM_MODEL_ID: "/data/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
TENSOR_PARALLEL_SIZE: 4
resources:
  limits:
    gpu.intel.com/i915: 4
#securityContext:
#  privileged: true

global:
  HUGGINGFACEHUB_API_TOKEN: "insert-your-huggingface-token-here"
  # set modelUseHostPath or modelUsePVC to use model cache.
  modelUseHostPath: /opt/opea-offline-data/models
  # modelUsePVC: model-PVC
