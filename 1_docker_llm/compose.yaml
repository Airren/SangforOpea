services:
  vllm-service:
    image: ${DOCKER_IMAGE}
    container_name: ${LLM_CONTAINER_NAME}
    ports:
      - "9009:80"
    ipc: host
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - "${MODELS_PATH}:/data"
    shm_size: ${SHM_SIZE:-8g}
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
      LLM_MODEL_LOCAL_PATH: ${LLM_MODEL_LOCAL_PATH}
      VLLM_TORCH_PROFILER_DIR: "/mnt"
      DTYPE: ${DTYPE:-float16}
      QUANTIZATION: ${QUANTIZATION:-fp8}
      MAX_MODEL_LEN: ${MAX_MODEL_LEN:-2048}
      MAX_NUM_BATCHED_TOKENS: ${MAX_NUM_BATCHED_TOKENS:-4000}
      MAX_NUM_SEQS: ${MAX_NUM_SEQS:-256}
      TENSOR_PARALLEL_SIZE: ${TENSOR_PARALLEL_SIZE:-1}
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://$host_ip:9009/health || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 100
    entrypoint: /bin/bash -c "export CCL_WORKER_COUNT=2 &&
      export SYCL_CACHE_PERSISTENT=1 &&
      export FI_PROVIDER=shm &&
      export CCL_ATL_TRANSPORT=ofi &&
      export CCL_ZE_IPC_EXCHANGE=sockets &&
      export CCL_ATL_SHM=1 &&
      export USE_XETLA=OFF &&
      export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2 &&
      export TORCH_LLM_ALLREDUCE=0 &&
      export CCL_SAME_STREAM=1 &&
      export CCL_BLOCKING_WAIT=0 &&
      source /opt/intel/1ccl-wks/setvars.sh &&
      python -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \
      --served-model-name $LLM_MODEL_ID \
      --port 80 \
      --model $LLM_MODEL_LOCAL_PATH \
      --trust-remote-code \
      --block-size 8 \
      --gpu-memory-utilization 0.95 \
      --device xpu \
      --dtype $DTYPE \
      --enforce-eager \
      --load-in-low-bit $QUANTIZATION \
      --max-model-len $MAX_MODEL_LEN \
      --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \
      --max-num-seqs $MAX_NUM_SEQS \
      --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
      --disable-async-output-proc \
      --distributed-executor-backend ray"